{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing with `beautifulsoup4`\n",
    "\n",
    "Previously, we have seen how to obtain the HTML content of a webpage using the `requests` library. But the HTML content is not very useful in its raw form. We need to parse it to extract the information we need. This is where the `beautifulsoup4` library comes in. \n",
    "\n",
    "## What is `beautifulsoup4`?\n",
    "\n",
    "`beautifulsoup4` is a Python library that is used to parse HTML and XML documents. It creates a parse tree from the page source code that can be used to extract data easily. It is a very powerful library and can be used to extract data from webpages in a very efficient manner.\n",
    "\n",
    "## Parsing using regular expressions\n",
    "\n",
    "Let's try to parse the HTML content of a webpage without using `beautifulsoup4`. We will use the `requests` library to obtain the HTML content of the webpage and then use the `re` library to parse the content.\n",
    "\n",
    "I assume that you are familiar with regular expressions. Perhaps you have visited the concept in your automata theory class. If not, you can learn about regular expressions from the [Python documentation](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en-US\">\\n    <head>\\n        <meta charset=\"UTF-8\">\\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n        <link rel=\"icon\" href=\"/wp-content/uploads/2022/11/Bicol_University-1.png\" sizes=\"any\">\\n                <link rel=\"apple-touch-icon\" href=\"/wp-content/themes/yootheme/packages/theme-wordpress/assets/images/apple-touch-icon.png\">\\n                <title>Search Results for &#8220;computer science&#8221; &#8211; Official Website of Bicol University</title>\\n<meta name=\\'robots\\' content=\\'noindex, follow, max-image-preview:large\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//translate.google.com\\' />\\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Official Website of Bicol University &raquo; Feed\" href=\"https://bicol-u.edu.ph/feed/\" />\\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Official Website of Bicol University &raquo; Comments Feed\" href=\"https://bicol-u.edu.ph/comments/feed/\" />\\n<link rel=\"alternate\" type=\"application/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "a_tag = \"https://bicol-u.edu.ph/?s=computer+science\"\n",
    "response = requests.get(a_tag)\n",
    "html = response.text\n",
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all links from the webpage using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://bicol-u.edu.ph/dswd-v-to-offer-internship-program-for-buenos/',\n",
       " 'https://bicol-u.edu.ph/ceng-mentors-off-to-international-academic-visit-cultural-immersion-in-taiwan/',\n",
       " 'https://bicol-u.edu.ph/buenos-return-after-a-successful-international-student-exchange-program-in-korea-taiwan/',\n",
       " 'https://bicol-u.edu.ph/bucs-comsci-it-dept-goes-benchmarking/',\n",
       " 'https://bicol-u.edu.ph/bu-nqu-language-exchange-project-commences-20-buenos-to-learn-chinese-mandarin/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'<a class=\"uk-link-reset\" href=\"(.+?)\">')\n",
    "links = pattern.findall(html)\n",
    "links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pattern = re.compile(r'<a class=\"uk-link-reset\" href=\"(.+?)\">')\n",
    "links = pattern.findall(html)\n",
    "links[:5]\n",
    "```\n",
    "\n",
    "Let me explain the code:\n",
    "\n",
    "1. A regular expression pattern is created to match the links of the website posts.\n",
    "\n",
    "    - `<a class=\"uk-link-reset\"`: This is the start of the anchor tag. Reviewing the HTML content of the webpage, we can see that all links are inside an anchor tag with this class.\n",
    "    - `href=\"`: This is the start of the `href` attribute of the anchor tag.\n",
    "    - `(.+?)`: This is a non-greedy match for the `href` attribute. It will match everything until the next `\"` is encountered.\n",
    "    - `\">`: This is the end of the `href` attribute.\n",
    "\n",
    "2. `findall()` method is used to find all matches of the pattern in the HTML content.\n",
    "\n",
    "3. The first 5 links are printed.\n",
    "\n",
    "Does this look complicated? Yes, it does. Regular expressions are powerful, but they are not the best tool for parsing HTML content. They are not very readable and can be difficult to maintain. Thankfully, we have `beautifulsoup4` to help us with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing using `beautifulsoup4`\n",
    "\n",
    "The library makes it very easy to parse HTML content. Let's see how we can extract all links from the webpage using `beautifulsoup4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bicol-u.edu.ph/dswd-v-to-offer-internship-program-for-buenos/\n",
      "https://bicol-u.edu.ph/ceng-mentors-off-to-international-academic-visit-cultural-immersion-in-taiwan/\n",
      "https://bicol-u.edu.ph/buenos-return-after-a-successful-international-student-exchange-program-in-korea-taiwan/\n",
      "https://bicol-u.edu.ph/bucs-comsci-it-dept-goes-benchmarking/\n",
      "https://bicol-u.edu.ph/bu-nqu-language-exchange-project-commences-20-buenos-to-learn-chinese-mandarin/\n",
      "https://bicol-u.edu.ph/student/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "a_tags = soup.find_all(\"a\", class_=\"uk-link-reset\")\n",
    "for a in a_tags:\n",
    "    print(a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works as follows:\n",
    "\n",
    "1. `BeautifulSoup` is imported from the `bs4` library.\n",
    "2. The HTML content is passed to the `BeautifulSoup` constructor, along with the parser to be used. In this case, we are using the `html.parser` parser.\n",
    "3. The `find_all()` method is used to find all anchor tags with the class `uk-link-reset`.\n",
    "4. A list is created to store the links.\n",
    "5. The `get()` method is used to extract the `href` attribute of each anchor tag and append it to the list.\n",
    "6. The first 5 links are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting meaningful information\n",
    "\n",
    "We have seen how `beautifulsoup4` can simplify our parsing tasks. But we have only extracted the links from the webpage. What if we want to extract more meaningful information, such as the title of the posts, metadata, and summary? We can do that using `beautifulsoup4` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSWD V to offer internship program for BUeños\n",
      "https://bicol-u.edu.ph/dswd-v-to-offer-internship-program-for-buenos/\n",
      "To learn through actual work experience, the Department of Social Welfare and Development (DSWD) Field Office V will provide an on-the-job training (OJT) program to select BUeños, formalized through a Memorandum of Agreement signed with Bicol University (BU).  DSWD will accommodate BUeños who will undergo field instruction (FI), field observation (FO), and/or OJT from the […]\n",
      "\n",
      "CENG Mentors Off to International Academic Visit, Cultural Immersion in Taiwan\n",
      "https://bicol-u.edu.ph/ceng-mentors-off-to-international-academic-visit-cultural-immersion-in-taiwan/\n",
      "\n",
      "\n",
      "BUeños return after a successful international student exchange program in Korea & Taiwan\n",
      "https://bicol-u.edu.ph/buenos-return-after-a-successful-international-student-exchange-program-in-korea-taiwan/\n",
      "Three Bicol University students return, having completed two international student exchange programs for the Global Korea Scholarship at Dongseo University, South Korea, and the Youth Elite Exchange Program under the New Southbound Policy International Study Program at National Quemoy University, Taiwan. Summer Jan Forrest Untalan, spent nearly four months in South Korea, and John Olan […]\n",
      "\n",
      "BUCS ComSci, IT dept. goes benchmarking\n",
      "https://bicol-u.edu.ph/bucs-comsci-it-dept-goes-benchmarking/\n",
      "\n",
      "\n",
      "BU-NQU Language Exchange Project commences; 20 BUeños to learn Chinese Mandarin\n",
      "https://bicol-u.edu.ph/bu-nqu-language-exchange-project-commences-20-buenos-to-learn-chinese-mandarin/\n",
      "Tuesday September 6,2022 o enhance the teaching-learning practices while fostering cross-cultural understanding and communicative competence, Bicol University (BU) and its international partner, National Quemoy University (NQU), virtually inaugurated an interactive online Language Exchange Project on September 5, 2022. The program will run for 15 weeks, or 30 hours, and will provide BU and NQU students […]\n",
      "\n",
      "Student\n",
      "https://bicol-u.edu.ph/student/\n",
      "Student Office of Student Affairs and Services The Office of Student Affairs and Services (OSAS) of the Bicol University is in charge of the provision of the Student Personnel Services (SPS), an integrated comprehensive system of co-curricular services and activities aimed at contributing to the fulfillment of the University’s mission statement. The OSS works alongside […]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_tag = \"https://bicol-u.edu.ph/?s=computer+science\"\n",
    "response = requests.get(a_tag)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "main_content = soup.find(\"main\")\n",
    "articles = main_content.find_all(\"article\")\n",
    "\n",
    "for article in articles:\n",
    "    title = article.find(\"h2\").text.strip()\n",
    "    a_tag = article.find(\"a\").get(\"href\")\n",
    "    summary = article.find(\"div\", class_=\"uk-margin-medium\").text.strip()\n",
    "    print(f\"{title}\\n{a_tag}\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps followed in the code:\n",
    "\n",
    "1. The `requests` and `BeautifulSoup` libraries are imported.\n",
    "2. The HTML content of the webpage is obtained using the `requests` library.\n",
    "3. A `BeautifulSoup` object is created to parse the HTML content.\n",
    "4. The main content of the webpage is extracted using the `find()` method.\n",
    "5. All articles containing the posts are extracted using the `find_all()` method.\n",
    "6. For each article in the list of articles, the title, link, and summary are extracted and printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Getting to Philosophy\n",
    "\n",
    "**How many pages does it take to get to the \"Philosophy\" page on Wikipedia?**\n",
    "\n",
    "This is a fun experiment that you can do using `beautifulsoup4`. The experiment is based on the observation that if you click on the first link in the main text of a Wikipedia page and then repeat the process for subsequent pages, you will eventually reach the \"Philosophy\" page. This is known as the \"Getting to Philosophy\" phenomenon. There is even a dedicated [Wikipedia page](https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy) for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://en.wikipedia.org/wiki/Independent_film\n",
      "Scraping: https://en.wikipedia.org/wiki/Feature_film\n",
      "Scraping: https://en.wikipedia.org/wiki/Narrative_film\n",
      "Scraping: https://en.wikipedia.org/wiki/Motion_picture\n",
      "Scraping: https://en.wikipedia.org/wiki/Visual_art\n",
      "Scraping: https://en.wikipedia.org/wiki/Art#Forms,_genres,_media,_and_styles\n",
      "Scraping: https://en.wikipedia.org/wiki/Human_behavior\n",
      "Scraping: https://en.wikipedia.org/wiki/Energy_(psychological)\n",
      "Scraping: https://en.wikipedia.org/wiki/Psychology\n",
      "Scraping: https://en.wikipedia.org/wiki/Mind\n",
      "Scraping: https://en.wikipedia.org/wiki/Thought\n",
      "Scraping: https://en.wikipedia.org/wiki/Consciousness\n",
      "Scraping: https://en.wikipedia.org/wiki/Awareness\n",
      "Scraping: https://en.wikipedia.org/wiki/Philosophy\n",
      "Philosophy reached in 13 steps\n"
     ]
    }
   ],
   "source": [
    "def is_valid_link(a_tag):\n",
    "    \"\"\"\n",
    "    Checks if an anchor tag is a valid link to follow.\n",
    "\n",
    "    A link is valid if it is not empty, does not contain parentheses, and is not italicized.\n",
    "\n",
    "    Args:\n",
    "        `a_tag` (bs4.element.Tag): The anchor tag to check.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        a_tag.text != \"\"\n",
    "        and \"(\" not in a_tag.text\n",
    "        and \")\" not in a_tag.text\n",
    "        and not a_tag.has_attr(\"italic\")\n",
    "    )\n",
    "\n",
    "\n",
    "def get_first_valid_link(url):\n",
    "    \"\"\"\n",
    "    Scrapes a Wikipedia page and returns the URL of the first valid link on the page.\n",
    "\n",
    "    Args:\n",
    "        `url` (str): The URL of the Wikipedia page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        `str` or `None`: The URL of the first valid link on the page, or None if no valid links are found.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content = soup.find(id=\"mw-content-text\")\n",
    "    paragraphs = content.find_all(\"p\")\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        links = paragraph.find_all(\"a\", href=True)\n",
    "        for link in links:\n",
    "            if not is_valid_link(link):\n",
    "                continue\n",
    "            return \"https://en.wikipedia.org\" + link[\"href\"]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_until_philosophy(start_url, max_steps=50):\n",
    "    \"\"\"\n",
    "    Scrapes Wikipedia pages, following the first valid link on each page, until the Philosophy page is reached.\n",
    "\n",
    "    Args:\n",
    "        `start_url` (str): The URL of the Wikipedia page to start scraping from.\n",
    "        `max_steps` (int): The maximum number of steps to take before the process times out.\n",
    "\n",
    "    Returns:\n",
    "        `int`: The number of steps it took to get to the Philosophy page, or -1 if the process times out.\n",
    "    \"\"\"\n",
    "    goal_link = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "    next_link = start_url\n",
    "    for i in range(max_steps):\n",
    "        print(f\"Scraping: {next_link}\")\n",
    "        if next_link == goal_link:\n",
    "            return i\n",
    "        next_link = get_first_valid_link(next_link)\n",
    "        if next_link is None:\n",
    "            return -1\n",
    "    return -1\n",
    "\n",
    "\n",
    "start_link = \"https://en.wikipedia.org/wiki/Independent_film\"\n",
    "visit_count = scrape_until_philosophy(start_link)\n",
    "print(f\"Philosophy reached in {visit_count} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will leave the explanation of the code as an exercise for you. Try changing some parts of the code and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge\n",
    "\n",
    "The code for the \"Getting to Philosophy\" experiment doesn't always work. For example, it fails under this link: [https://en.wikipedia.org/wiki/Special:Random](https://en.wikipedia.org/wiki/Special:Random).\n",
    "\n",
    "It seems that there are some pages on Wikipedia that don't lead to the \"Philosophy\" page. Can you modify the code to handle this? Begin by opening the link in a web browser and inspecting the HTML content. Is there a pattern that you can use to identify pages that don't lead to the \"Philosophy\" page? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://en.wikipedia.org/wiki/Special:Random\n",
      "Scraping: https://en.wikipedia.org/wiki/Great_Britain\n",
      "Scraping: https://en.wikipedia.org/wiki/Island\n",
      "Scraping: https://en.wikipedia.org/wiki/Atoll\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Scraping: https://en.wikipedia.org/wiki/Symbol\n",
      "Scraping: https://en.wikipedia.org/wiki/Sign_(semiotics)\n",
      "Scraping: https://en.wikipedia.org/wiki/Semiotics\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Scraping: https://en.wikipedia.org/wiki/Symbol\n",
      "Scraping: https://en.wikipedia.org/wiki/Sign_(semiotics)\n",
      "Scraping: https://en.wikipedia.org/wiki/Semiotics\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Scraping: https://en.wikipedia.org/wiki/Symbol\n",
      "Scraping: https://en.wikipedia.org/wiki/Sign_(semiotics)\n",
      "Scraping: https://en.wikipedia.org/wiki/Semiotics\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Scraping: https://en.wikipedia.org/wiki/Symbol\n",
      "Scraping: https://en.wikipedia.org/wiki/Sign_(semiotics)\n",
      "Scraping: https://en.wikipedia.org/wiki/Semiotics\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Scraping: https://en.wikipedia.org/wiki/Symbol\n",
      "Scraping: https://en.wikipedia.org/wiki/Sign_(semiotics)\n",
      "Scraping: https://en.wikipedia.org/wiki/Semiotics\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Scraping: https://en.wikipedia.org/wiki/Symbol\n",
      "Scraping: https://en.wikipedia.org/wiki/Sign_(semiotics)\n",
      "Scraping: https://en.wikipedia.org/wiki/Semiotics\n",
      "Scraping: https://en.wikipedia.org/wiki/Help:IPA/English\n",
      "Scraping: https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Alphabet\n",
      "Scraping: https://en.wikipedia.org/wiki/Letter_(alphabet)\n",
      "Philosophy reached in -1 steps\n"
     ]
    }
   ],
   "source": [
    "random_page = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "visit_count = scrape_until_philosophy(random_page)\n",
    "print(f\"Philosophy reached in {visit_count} steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
